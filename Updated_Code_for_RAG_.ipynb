{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 1: Install Required Libraries\n",
        "\n",
        "First, ensure you have the necessary libraries installed. In Google Colab, you can run shell commands by prefixing them with `!`."
      ],
      "metadata": {
        "id": "EcFUQyv47uWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "HsY8Ab3Q7ygr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 2:  **Import Libraries**\n",
        "\n",
        "\n",
        "\n",
        "Import the required classes from the `transformers` library."
      ],
      "metadata": {
        "id": "wRPeaTkK73bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration"
      ],
      "metadata": {
        "id": "MJG_pQJI77jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **RagTokenizer**: Tokenizes input text for the RAG model.\n",
        "\n",
        "- **RagRetriever**: Retrieves relevant documents for the input query.\n",
        "\n",
        "- **RagSequenceForGeneration**: Generates responses based on the retrieved documents.\n"
      ],
      "metadata": {
        "id": "3cNlF_LvHbF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Step 3. **Load Pre-trained Models**\n",
        "Load the pre-trained RAG model, tokenizer, and retriever."
      ],
      "metadata": {
        "id": "8SVj9QimF2iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer, retriever, and model\n",
        "tokenizer = RagTokenizer.from_pretrained('facebook/rag-sequence-nq')"
      ],
      "metadata": {
        "id": "Xk25dA6XF7JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Pre-trained Model**: `facebook/rag-sequence-nq` is a pre-trained RAG model fine-tuned on the Natural Questions dataset."
      ],
      "metadata": {
        "id": "Oy36HPSKHo1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy dataset\n",
        "dummy_dataset = [\n",
        "    {\"title\": \"RAG Model\", \"text\": \"RAG stands for Retrieval-Augmented Generation. It combines retrieval and generation to answer questions.\"},\n",
        "    {\"title\": \"Transformers Library\", \"text\": \"The Transformers library by Hugging Face provides state-of-the-art machine learning models for NLP tasks.\"},\n",
        "    {\"title\": \"Natural Questions Dataset\", \"text\": \"The Natural Questions dataset is a large-scale dataset for training models to answer real-world questions.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "vjaMzhURlbFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the retriever with the dummy dataset\n",
        "retriever = RagRetriever.from_pretrained(\n",
        "    'facebook/rag-sequence-nq',\n",
        "    index_name=\"exact\",\n",
        "    passages=dummy_dataset\n",
        ")\n",
        "\n",
        "model = RagSequenceForGeneration.from_pretrained('facebook/rag-sequence-nq')"
      ],
      "metadata": {
        "id": "HkcIZhiglj1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://media.licdn.com/dms/image/v2/D4D12AQEmnZaGnImACg/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1713229836462?e=2147483647&v=beta&t=UrNxjrGyauZshhkc9SwMBqUjfBdrdTqaMhirOUUAP9M)"
      ],
      "metadata": {
        "id": "ntWuwL5hsIwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 4: **Generate a Response**\n",
        "Tokenize the input text, generate a response using the model, and decode the output."
      ],
      "metadata": {
        "id": "DL1VlPlHCast"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the input text, generate a response using the model, and decode the output.\n",
        "input_text = \"What is RAG?\"\n",
        "\n",
        "# Tokenize the input text\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# Retrieve relevant documents\n",
        "retrieved_docs = retriever(input_ids=input_ids, return_tensors=\"pt\")\n",
        "print(\"Retrieved Documents:\", retrieved_docs)\n",
        "\n",
        "# Generate a response\n",
        "generated = model.generate(input_ids, num_beams=2, num_return_sequences=1)\n",
        "print(\"Generated Token IDs:\", generated)\n",
        "\n",
        "# Decode the generated response\n",
        "output = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
        "\n",
        "# Print the output\n",
        "print(\"Generated Response:\", output[0])"
      ],
      "metadata": {
        "id": "SDNIwc_mCf11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Key Points to Understand RAG:\n",
        "\n",
        "1. **Tokenization**: The input text is tokenized into a format that the model can process.\n",
        "\n",
        "2. **Retrieval**: The `RagRetriever` retrieves relevant documents from the dummy dataset based on the input query. This step is crucial as it shows the retrieval part of RAG.\n",
        "\n",
        "3. **Generation**: The `RagSequenceForGeneration` model generates a response using the retrieved documents. This is the generation part of RAG.\n",
        "\n",
        "4. **Decoding**: The generated token IDs are converted back into human-readable text.\n",
        "\n",
        "By printing the retrieved documents and the generated token IDs, you can see how the RAG model is working step-by-step. This will help you understand the retrieval and generation process in action."
      ],
      "metadata": {
        "id": "ZU5ZVyJimYZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Tokenization**: Converts the input text into a format suitable for the model.\n",
        "\n",
        "- **Generation**: The model generates a response using beam search (`num_beams=2`) to explore multiple possible outputs.\n",
        "\n",
        "- **Decoding**: Converts the generated token IDs back into human-readable text."
      ],
      "metadata": {
        "id": "EN-xgyqAH3nA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Additional Resources\n",
        "\n",
        "\n",
        "   - [Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
        "   \n",
        "   - [RAG Paper](https://arxiv.org/abs/2005.11401)"
      ],
      "metadata": {
        "id": "kh8AN6BY8Wjb"
      }
    }
  ]
}